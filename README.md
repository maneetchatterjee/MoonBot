# Agile Roverâ€“Manipulator Reinforcement Learning Framework

Simulation-first development of an autonomous rover with an articulated robotic arm using physics-based reinforcement learning.

This repository presents a **complete end-to-end robotics learning pipeline**, starting from **CAD-level mechanical design** and culminating in **robust locomotion policies trained via reinforcement learning** in high-fidelity physics simulators. The framework is **simulator-agnostic**, **scalable**, and **sim-to-real ready**.

---

## ğŸš€ Motivation

Modern mobile manipulators must operate under:
- Uncertain terrain conditions
- Strict stability constraints
- Energy-efficient actuation
- Modeling inaccuracies and noise

Rather than iterating directly on hardware, this project adopts a **simulation-driven learning approach**, enabling:
- Safe large-scale exploration
- Rapid policy iteration
- Domain randomization
- Transferable control policies

---

## ğŸ§  Key Contributions

- Full **CAD â†’ URDF/Xacro â†’ Physics Simulation** pipeline
- **Gymnasium-compatible reinforcement learning environments**
- **PPO-based locomotion training** with stability-aware reward shaping
- **Domain randomization** for sim-to-real robustness
- **Hierarchical RLâ€“ready architecture** (locomotion + manipulation)
- Cross-simulator compatibility: **PyBullet, MuJoCo, Isaac Sim**

---

## ğŸ¦¾ Robot Overview

- **Platform**: 6-wheel rover with articulated suspension
- **Manipulator**: Multi-DOF robotic arm mounted on rover base
- **Control**:
  - Locomotion: Wheel velocity control
  - Manipulation: Joint-space / task-space control (extensible)
- **Simulated Sensors**:
  - IMU (base orientation, angular velocity)
  - Wheel encoders
  - Base linear velocity
  - Extendable to RGB-D / LiDAR

---

## ğŸ“ Repository Structure

agile-rover-rl/
â”œâ”€â”€ assets/ # Visual & collision meshes
â”‚ â”œâ”€â”€ meshes/
â”‚ â”‚ â”œâ”€â”€ visual/
â”‚ â”‚ â””â”€â”€ collision/
â”‚
â”œâ”€â”€ urdf/ # Robot description files
â”‚ â”œâ”€â”€ rover_base.xacro
â”‚ â”œâ”€â”€ rover_arm.xacro
â”‚ â””â”€â”€ rover.urdf
â”‚
â”œâ”€â”€ envs/ # Gymnasium environments
â”‚ â”œâ”€â”€ rover_locomotion_env.py
â”‚ â”œâ”€â”€ rover_arm_env.py
â”‚ â””â”€â”€ reward_functions.py
â”‚
â”œâ”€â”€ rl/ # Reinforcement learning utilities
â”‚ â”œâ”€â”€ domain_randomization.py
â”‚ â”œâ”€â”€ callbacks.py
â”‚ â””â”€â”€ policies.py
â”‚
â”œâ”€â”€ train/ # Training scripts
â”‚ â”œâ”€â”€ train_locomotion_ppo.py
â”‚ â”œâ”€â”€ train_hierarchical.py
â”‚ â””â”€â”€ hyperparams.yaml
â”‚
â”œâ”€â”€ eval/ # Evaluation & metrics
â”‚ â”œâ”€â”€ evaluate_policy.py
â”‚ â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ utils/ # Supporting utilities
â”‚ â”œâ”€â”€ mesh_utils.py
â”‚ â””â”€â”€ urdf_utils.py
â”‚
â”œâ”€â”€ results/ # Logs, checkpoints, plots
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



---

## ğŸ”§ Installation

### 1. Create Python Environment
```bash
python -m venv rover_rl
source rover_rl/bin/activate      # Linux / macOS
# rover_rl\Scripts\activate       # Windows
pip install -r requirements.txt

ğŸ§± CAD â†’ Simulation Pipeline

Mechanical Design

Full rover and manipulator designed in SolidWorks

Mesh Processing

Visual meshes: High-resolution (DAE / OBJ)

Collision meshes: Simplified STL

Unit conversion (mm â†’ m), origin alignment, decimation

URDF/Xacro Modeling

Accurate inertial tensors

Joint limits and actuation constraints

Modular base and arm description

Physics Simulation

PyBullet for rapid iteration

MuJoCo for high-fidelity contact dynamics

Isaac Sim for photorealism and sim-to-real transfer

ğŸ¤– Reinforcement Learning Formulation
Observation Space

Base linear velocity (3)

Base angular velocity (3)

Orientation (roll, pitch, yaw)

Wheel joint positions

Action Space

Continuous wheel velocity commands (4 DOF)

Reward Function (Locomotion)

The reward balances:

Forward velocity maximization

Energy efficiency

Roll/pitch stability

Suppression of lateral and vertical motion

R = 2.0Â·v_x âˆ’ 2.5Â·(|roll| + |pitch|)
    âˆ’ 0.001Â·||a||Â² âˆ’ 0.3Â·|v_y| âˆ’ 0.5Â·|v_z|


ğŸ§ª Training
Locomotion Training (PPO)
python train/train_locomotion_ppo.py


Algorithm: Proximal Policy Optimization (PPO)

Policy: Multi-layer perceptron

Training horizon: 600k+ timesteps

Randomized physics parameters per episode

ğŸ” Domain Randomization

To improve sim-to-real transfer:

Link mass randomization (Â±15%)

Friction coefficient randomization

Motor strength scaling

Actuation noise injection

ğŸ§  Extensibility

The framework supports:

Hierarchical reinforcement learning

Vision-based end-to-end policies

Curriculum learning

Multi-objective rewards

Multi-agent extensions

Real-world deployment

ğŸŒ Sim-to-Real Outlook

The architecture is designed to facilitate:

Conservative collision modeling

Latency-aware actuation

Parameter identification

Safe hardware deployment

ğŸ›  Tools & Technologies

CAD & Modeling: SolidWorks, Blender, MeshLab

Simulation: PyBullet, MuJoCo, NVIDIA Isaac Sim

Learning: Stable-Baselines3 (PPO), Gymnasium

Programming: Python

Logging & Analysis: NumPy, TensorBoard

ğŸ“Œ Future Work

Full MuJoCo MJCF implementation

Hierarchical RL (locomotion + manipulation)

Vision-based perception policies

Isaac Sim photorealistic training

Real-robot validation

ğŸ“„ License

MIT License

ğŸ“¬ Contact

For collaboration, research extensions, or academic discussion, feel free to reach out.
